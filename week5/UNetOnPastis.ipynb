{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ff433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTISSegmentation(Dataset):\n",
    "    \"\"\"\n",
    "    Here we use a subset of the PASTIS dataset: https://github.com/VSainteuf/pastis-benchmark\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        annotation_dir: str,\n",
    "        split:str = \"train\",\n",
    "        median_of_days: bool = False,\n",
    "        Xmean = None,\n",
    "        Xstd = None,\n",
    "        binary_labels: bool = False,\n",
    "        normalize:bool = True,\n",
    "        transform = None,\n",
    "        device = 'cpu'\n",
    "    ) -> None:\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        images = glob(os.path.join(image_dir, split, 'S2_*.npy'))\n",
    "        self.images = images\n",
    "        \n",
    "        annotations = []\n",
    "        for im in images:\n",
    "            name = os.path.splitext(os.path.basename(im))[0].replace(\"S2_\", \"\")\n",
    "            annotations.append(os.path.join(annotation_dir, split, f\"TARGET_{name}.npy\"))\n",
    "        self.annotations = annotations\n",
    "        # Store in the class for future reference\n",
    "        self.median_of_days = median_of_days\n",
    "        self.binary_labels = binary_labels\n",
    "        \n",
    "        self.Xmean  = Xmean\n",
    "        self.Xstd = Xstd\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_data(self, files, median, norm=True):\n",
    "        \"\"\"\n",
    "        Reads and stacks our data\n",
    "        \"\"\"\n",
    "        t = []\n",
    "        for im in files:\n",
    "            r = np.load(im)\n",
    "            if median:\n",
    "                r = np.median(r, axis=0) #Take median value across 43 days\n",
    "            if norm and self.Xmean is not None and  self.Xstd is not None:\n",
    "                r = (r - self.Xmean) / self.Xstd\n",
    "            t.append(r)\n",
    "        return np.stack(t, axis=0)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = self.read_data([self.images[idx]], self.median_of_days)[0]\n",
    "        y =  (self.read_data([self.annotations[idx]], median=False, norm=False)[0,0]).astype(np.int32)\n",
    "\n",
    "        if self.binary_labels:\n",
    "            y[y>0] = 1 # Convert to binary labels\n",
    "            \n",
    "            sample = {\n",
    "             'X': torch.FloatTensor(X), \n",
    "             'y': torch.FloatTensor(y)\n",
    "            }\n",
    "        else:\n",
    "            sample = {'X': torch.FloatTensor(X), 'y': torch.LongTensor(y)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae1317",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_path = \"/home/ankit/Vision/Pastis/\" # Define it\n",
    "\n",
    "Xmean = np.array([ 596.57817383, 878.493514, 969.89764811, 1324.39628906, 2368.21767578, 2715.68257243, 2886.70323486, 2977.03915609, 2158.25386556, 1462.10965169])\n",
    "Xmean = Xmean.reshape((10, 1, 1))\n",
    "Xstd = np.array([251.33337853, 289.95055489, 438.725014, 398.7289996, 706.53781626, 832.72503267, 898.14189979, 909.04165075, 661.66078257, 529.15340992])\n",
    "Xstd = Xstd.reshape((10, 1, 1))\n",
    "\n",
    "p_train = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                             split=\"train\",\n",
    "                             median_of_days=True,\n",
    "                             Xmean=Xmean,\n",
    "                             Xstd=Xstd,\n",
    "                             binary_labels=False, \n",
    "                             transform = None)\n",
    "\n",
    "p_val = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                            split=\"val\",\n",
    "                            median_of_days=True,\n",
    "                            Xmean=Xmean,\n",
    "                            Xstd=Xstd,\n",
    "                            binary_labels=False, \n",
    "                            transform = None)\n",
    "\n",
    "p_test = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                            split=\"test\",\n",
    "                            median_of_days=True,\n",
    "                            Xmean=Xmean,\n",
    "                            Xstd=Xstd,\n",
    "                            binary_labels=False, \n",
    "                            transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader from the dataset\n",
    "# Dataloader gives us the possibility to sample a mini-batches instead of only a single sample\n",
    "BATCH_SIZE = 4 # Adjust the batch to fit the VRAM of the GPU\n",
    "# If your BATCH_SIZE is too small then compensate for it by increasing the grad_accumulation in the training step\n",
    "\n",
    "num_workers = 8 # Change depending upon the available hardware\n",
    "train_dataloader = DataLoader(p_train, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_dataloader = DataLoader(p_val, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=num_workers)\n",
    "\n",
    "test_dataloader = DataLoader(p_test, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dffc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_dataloader:\n",
    "    print(d['X'].shape, d['y'].shape, )\n",
    "    print(d['X'].dtype, d['y'].dtype, )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = 5\n",
    "# Show the 3rd band of the third image\n",
    "plt.imshow(p_train[imd]['X'][3].cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "# Show the labels for third image\n",
    "plt.imshow(p_train[imd]['y'].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d4880",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81acedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our neural network\n",
    "\n",
    "# Modified from: https://github.com/milesial/Pytorch-UNet\n",
    "# GPL-3 license\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ELU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation loop\n",
    "def eval_loop(model, val_loader, criterion):\n",
    "#     print(f\"Validating using the val_loader\")\n",
    "    epoch_loss_val = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            x, y_true = batch['X'].to(dtype=torch.float32, device=device), batch['y'].to(device=device)\n",
    "            y_true = torch.squeeze(y_true, dim=1)\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            ### Calcualte loss\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            epoch_loss_val.append(loss.item())\n",
    "    el = torch.mean(torch.FloatTensor(epoch_loss_val))\n",
    "    model.train()\n",
    "    return el\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(model, train_loader, val_loader, optimizer, criterion, epochs=50, grad_accumulation=1):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    n_train =  len(train_loader)\n",
    "    for e in range(epochs):\n",
    "        epoch_loss_train = []\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            print(f\"Training on Batch: {batch_idx}\")\n",
    "            x, y_true = batch['X'].to(dtype=torch.float32, device=device), batch['y'].to(device=device)\n",
    "            y_true = torch.squeeze(y_true, dim=1)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            ### Calcualte loss\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % grad_accumulation == 0 or (batch_idx + 1 == n_train):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss_train.append(loss.item())\n",
    "\n",
    "        el = torch.mean(torch.FloatTensor(epoch_loss_train))\n",
    "        print(f\"Train loss for epoch {e}: {el}\")\n",
    "        train_loss.append(el)\n",
    "        \n",
    "        vel = eval_loop(model, val_loader, criterion)\n",
    "        print(f\"Validation loss for epoch {e}: {vel}\")\n",
    "        val_loss.append(vel)\n",
    "        \n",
    "    return model, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our first model\n",
    "input_size = 10\n",
    "output_size = 20\n",
    "\n",
    "model = UNet(n_channels = input_size, n_classes = output_size)\n",
    "model.to(device=device)\n",
    "# Define the optimizer, the loss function and \n",
    "lr = 0.0001 # The learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr) # Optimizer calculates the gradients and use it to update the model weights. \n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "epochs = 20\n",
    "\n",
    "grad_accumulation = 4 # Accumulate grad over 4 batches\n",
    "\n",
    "#Train the model for 50 epochs with CrossEntropry loss.\n",
    "model, train_loss, val_loss = train_loop(model, train_dataloader, val_dataloader,  optimizer, criterion, epochs=epochs, grad_accumulation=grad_accumulation)\n",
    "print(f\"Trained for {epochs} epochs in {time.time()-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate the model performance on the validation dataset and plot the training curve\n",
    "#Visualize a convolutional kernel from the first convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0d0b2",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss functions (30 points)\n",
    "\n",
    "# Dataset: Same as task 1\n",
    "\n",
    "# Treat it as a regression task, where the model predicts a single value. \n",
    "# Train the model for 20 epochs with L2 loss. \n",
    "# Note: The model will require modifications since you want a single value output instead of probability \n",
    "# of each class. From the regressed value, use round(y_pred) to treat it as the class label. \n",
    "# For example, if the output is 14.2, treat it like the model predicted class 14 using round(14.2) method.\n",
    "\n",
    "# Validate the model and plot the training curves. Comment on your observations. \n",
    "# Visualize a convolutional kernel from the first convolutional layer.\n",
    "\n",
    "# Choose the better model of the two and evaluate on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc35e49",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff602f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Learning rate (40 points)\n",
    "\n",
    "# Summary: The importance of a correct learning rate\n",
    "\n",
    "# Dataset: Same as task 1\n",
    "# Train the model  for 50 epochs with a learning rate of 10.\n",
    "# Train the model  for 50 epochs with a learning rate of 1e-3.\n",
    "# Train the model  for 50 epochs with a learning rate of 1e-10.\n",
    "# Validate each model and plot each of the training curves. Comment on your observations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803696d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
