{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ff433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTISSegmentation(Dataset):\n",
    "    \"\"\"\n",
    "    Here we use a subset of the PASTIS dataset: https://github.com/VSainteuf/pastis-benchmark\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        annotation_dir: str,\n",
    "        split:str = \"train\",\n",
    "        median_of_days: bool = False,\n",
    "        Xmean = None,\n",
    "        Xstd = None,\n",
    "        binary_labels: bool = False,\n",
    "        normalize:bool = True,\n",
    "        transform = None\n",
    "    ) -> None:\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        images = glob(os.path.join(image_dir, split, 'S2_*.npy'))\n",
    "        annotations = []\n",
    "        for im in images:\n",
    "            name = os.path.splitext(os.path.basename(im))[0].replace(\"S2_\", \"\")\n",
    "            annotations.append(os.path.join(annotation_dir, split, f\"TARGET_{name}.npy\"))\n",
    "\n",
    "        # Store in the class for future reference\n",
    "        self.median_of_days = median_of_days\n",
    "        self.binary_labels = binary_labels\n",
    "        \n",
    "        \n",
    "        # Load data\n",
    "        self.X = self.read_data(images)\n",
    "        norm_dims = (0,1,3,4)\n",
    "        if median_of_days:\n",
    "            self.X = np.median(self.X, axis=1) #Take median value across 43 days\n",
    "            norm_dims = (0, 2, 3)\n",
    "        # Normalize the data if the normalization values are provided\n",
    "        if Xmean is not None and Xstd is not None:\n",
    "            self.X = (self.X - Xmean) / Xstd\n",
    "        \n",
    "        self.y = self.read_data(annotations)\n",
    "        self.y = self.y[:,0] # We are only interested in the 20 classes for now\n",
    "        if binary_labels:\n",
    "            self.y[self.y>0] = 1 # Convert to binary labels\n",
    "        self.x_pixel, self.y_pixel = self.pixelwise()\n",
    "        self.pixelwise_test()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x_pixel.shape[0]\n",
    "    \n",
    "    def read_data(self, files):\n",
    "        \"\"\"\n",
    "        Reads and stacks our data\n",
    "        \"\"\"\n",
    "        t = []\n",
    "        for im in files:\n",
    "#             print(self.split, np.load(im).shape)\n",
    "            t.append(np.load(im))\n",
    "        return np.stack(t, axis=0)\n",
    "        \n",
    "    def pixelwise(self):\n",
    "        \"\"\"\n",
    "        This method flattens our images to individual pixels, so we can treat\n",
    "        each pixel as a sample and train our favirote classifier on it. \n",
    "        \"\"\"\n",
    "        if self.median_of_days:\n",
    "            return  np.transpose(self.X, (0, 2, 3, 1)).reshape(-1, 10), self.y.reshape(-1)\n",
    "        else:\n",
    "            return  np.transpose(self.X, (0, 3, 4, 1, 2)).reshape(-1, 430), self.y.reshape(-1)\n",
    "    \n",
    "    def pixelwise_test(self):\n",
    "        \"\"\"\n",
    "        Short test for the above method.\n",
    "        \"\"\"\n",
    "        N = self.X.shape[0]\n",
    "        \n",
    "        pX, py = self.pixelwise()\n",
    "        if self.median_of_days:\n",
    "            tX = np.transpose(pX.reshape(N, 128, 128, 10), (0, 3, 1, 2))    \n",
    "        else:\n",
    "            tX = np.transpose(pX.reshape(N, 128, 128, 43, 10), (0, 3, 4, 1, 2))\n",
    "        tY = py.reshape(N, 128, 128)\n",
    "        assert np.all(tX == self.X) and np.all(tY == self.y)\n",
    "        print(\"All test passed!\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if self.binary_labels:\n",
    "            sample = {'X': torch.FloatTensor(self.x_pixel[idx]), 'y': torch.FloatTensor([self.y_pixel[idx]])} # 'y': torch.FloatTensor([self.y_pixel[idx]])}\n",
    "        else:\n",
    "            sample = {'X': torch.FloatTensor(self.x_pixel[idx]), 'y': torch.LongTensor([self.y_pixel[idx]])}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae1317",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_path = \"../\" # Define it\n",
    "\n",
    "Xmean = np.array([ 596.57817383, 878.493514, 969.89764811, 1324.39628906, 2368.21767578, 2715.68257243, 2886.70323486, 2977.03915609, 2158.25386556, 1462.10965169])\n",
    "Xmean = Xmean.reshape((1, 10, 1, 1))\n",
    "Xstd = np.array([251.33337853, 289.95055489, 438.725014, 398.7289996, 706.53781626, 832.72503267, 898.14189979, 909.04165075, 661.66078257, 529.15340992])\n",
    "Xstd = Xstd.reshape((1, 10, 1, 1))\n",
    "\n",
    "p_train = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                             split=\"train\",\n",
    "                             median_of_days=True,\n",
    "                             Xmean=Xmean,\n",
    "                             Xstd=Xstd,\n",
    "                             binary_labels=False, \n",
    "                             transform = None)\n",
    "\n",
    "p_val = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                            split=\"val\",\n",
    "                            median_of_days=True,\n",
    "                            Xmean=Xmean,\n",
    "                            Xstd=Xstd,\n",
    "                            binary_labels=False, \n",
    "                            transform = None)\n",
    "\n",
    "p_test = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                            split=\"test\",\n",
    "                            median_of_days=True,\n",
    "                            Xmean=Xmean,\n",
    "                            Xstd=Xstd,\n",
    "                            binary_labels=False, \n",
    "                            transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader from the dataset\n",
    "# Dataloader gives us the possibility to sample a mini-batches instead of only a single sample\n",
    "BATCH_SIZE = 4096\n",
    "train_dataloader = DataLoader(p_train, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(p_val, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(p_test, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dffc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_dataloader:\n",
    "    print(d['X'].shape, d['y'].shape, )\n",
    "    print(d['X'].dtype, d['y'].dtype, )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = 5\n",
    "# Show the 3rd band of the third image\n",
    "plt.imshow(p_train.X[imd, 3])\n",
    "plt.show()\n",
    "\n",
    "# Show the labels for third image\n",
    "plt.imshow(p_train.y[imd])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d4880",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81acedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our neural network\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, hidden_activations=None, output_activation=None):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Create a list of fully connected layers\n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if hidden_activations is not None:\n",
    "                layers.append(hidden_activations)  # Apply activation function on the hidden layers\n",
    "        \n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(layer_sizes[-1], output_size))\n",
    "        if output_activation is not None:\n",
    "                layers.append(output_activation)  # Apply activation function on the output layer\n",
    "         \n",
    "        # Combine all layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation loop\n",
    "def eval_loop(model, val_loader, criterion):\n",
    "#     print(f\"Validating using the val_loader\")\n",
    "    epoch_loss_val = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            x, y_true = batch['X'].to(dtype=torch.float32), batch['y']\n",
    "            y_true = torch.squeeze(y_true)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            ### Calcualte loss\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            epoch_loss_val.append(loss.item())\n",
    "    el = torch.mean(torch.FloatTensor(epoch_loss_val))\n",
    "    model.train()\n",
    "    return el\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(model, train_loader, val_loader, optimizer, criterion, epochs=50):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for e in range(epochs):\n",
    "        epoch_loss_train = []\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            x, y_true = batch['X'].to(dtype=torch.float32), batch['y']\n",
    "            y_true = torch.squeeze(y_true)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            ### Calcualte loss\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss_train.append(loss.item())\n",
    "\n",
    "        el = torch.mean(torch.FloatTensor(epoch_loss_train))\n",
    "        print(f\"Train loss for epoch {e}: {el}\")\n",
    "        train_loss.append(el)\n",
    "        \n",
    "        vel = eval_loop(model, val_loader, criterion)\n",
    "        print(f\"Validation loss for epoch {e}: {vel}\")\n",
    "        val_loss.append(vel)\n",
    "        \n",
    "    return model, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our first model\n",
    "input_size = 10\n",
    "hidden_sizes = [20, 30, 40]\n",
    "output_size = 20\n",
    "\n",
    "model = MLP(input_size, hidden_sizes, output_size, hidden_activations=nn.ReLU(), output_activation=None)\n",
    "\n",
    "# Define the optimizer, the loss function and \n",
    "lr = 0.0001 # The learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr) # Optimizer calculates the gradients and use it to update the model weights. \n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "epochs = 50\n",
    "model, train_loss, val_loss = train_loop(model, train_dataloader, val_dataloader,  optimizer, criterion, epochs=epochs)\n",
    "print(f\"Trained for {epochs} epochs in {time.time()-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our second model\n",
    "input_size = 10\n",
    "hidden_sizes = [2, 2, 2]\n",
    "output_size = 20\n",
    "\n",
    "model2 = MLP(input_size, hidden_sizes, output_size, hidden_activations=nn.ReLU(), output_activation=None)\n",
    "print(model2)\n",
    "\n",
    "# Define the optimizer, the loss function and \n",
    "lr = 0.0001 # The learning rate\n",
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=lr) # Optimizer calculates the gradients and use it to update the model weights. \n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model2\n",
    "start_time = time.time()\n",
    "epochs = 50\n",
    "model2, train_loss, val_loss = train_loop(model2, train_dataloader, val_dataloader,  optimizer, criterion, epochs=epochs)\n",
    "print(f\"Trained for {epochs} epochs in {time.time()-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training and validation loss as a function of epoch for the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6950c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy, precision, and recall on the test set (or using test_dataloader).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment on the accuracy, recall, and precision of the two MLP models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
