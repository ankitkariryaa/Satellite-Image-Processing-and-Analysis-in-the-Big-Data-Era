{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ff433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTISSegmentation:\n",
    "    \"\"\"\n",
    "    Here we use a subset of the PASTIS dataset: https://github.com/VSainteuf/pastis-benchmark\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        annotation_dir: str,\n",
    "        split:str = \"train\",\n",
    "        median_of_days: bool = False,\n",
    "        Xmean = None,\n",
    "        Xstd = None,\n",
    "        binary_labels: bool = False\n",
    "    ) -> None:\n",
    "        \n",
    "        images = glob(os.path.join(image_dir, split, 'S2_*.npy'))\n",
    "        annotations = []\n",
    "        for im in images:\n",
    "            name = os.path.splitext(os.path.basename(im))[0].replace(\"S2_\", \"\")\n",
    "            annotations.append(os.path.join(annotation_dir, split, f\"TARGET_{name}.npy\"))\n",
    "\n",
    "        # Store in the class for future reference\n",
    "        self.median_of_days = median_of_days\n",
    "        self.binary_labels = binary_labels\n",
    "        \n",
    "        # Load data\n",
    "        self.X = self.read_data(images)\n",
    "        if median_of_days:\n",
    "            self.X = np.median(self.X, axis=1) #Take median value across 43 days\n",
    "\n",
    "        ### Normalization\n",
    "        ### Using the provided values (Xmean, Xstd) we normalize the input X to have zero mean and unit variance\n",
    "        if Xmean is not None and Xstd is not None:\n",
    "            self.X = (self.X - Xmean) / Xstd\n",
    "            \n",
    "        self.y = self.read_data(annotations)\n",
    "        self.y = self.y[:,0] # We are only interested in the 20 classes for now\n",
    "        if binary_labels:\n",
    "            self.y[self.y>0] = 1 # Convert to binary labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def read_data(self, files):\n",
    "        \"\"\"\n",
    "        Reads and stacks our data\n",
    "        \"\"\"\n",
    "        t = []\n",
    "        for im in files:\n",
    "            t.append(np.load(im))\n",
    "        return np.stack(t, axis=0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def pixelwise(self):\n",
    "        \"\"\"\n",
    "        This method flattens our images to individual pixels, so we can treat\n",
    "        each pixel as a sample and train our favirote classifier on it. \n",
    "        \"\"\"\n",
    "        if self.median_of_days:\n",
    "            return  np.transpose(self.X, (0, 2, 3, 1)).reshape(-1, 10), self.y.reshape(-1)\n",
    "        else:\n",
    "            return  np.transpose(self.X, (0, 3, 4, 1, 2)).reshape(-1, 430), self.y.reshape(-1)\n",
    "    \n",
    "    def pixelwise_test(self):\n",
    "        \"\"\"\n",
    "        Short test for the above method.\n",
    "        \"\"\"\n",
    "        N = self.__len__()\n",
    "        \n",
    "        pX, py = self.pixelwise()\n",
    "        if self.median_of_days:\n",
    "            tX = np.transpose(pX.reshape(N, 128, 128, 10), (0, 3, 1, 2))    \n",
    "        else:\n",
    "            tX = np.transpose(pX.reshape(N, 128, 128, 43, 10), (0, 3, 4, 1, 2))\n",
    "        tY = py.reshape(N, 128, 128)\n",
    "        assert np.all(tX == self.X) and np.all(tY == self.y)\n",
    "        print(\"All test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../\" # Define it\n",
    "\n",
    "p_train = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                             split=\"train\",\n",
    "                             median_of_days=True,\n",
    "                             binary_labels=False)\n",
    "\n",
    "p_test = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                            split=\"test\",\n",
    "                            median_of_days=True,\n",
    "                            binary_labels=False)\n",
    "p_train.pixelwise_test()\n",
    "p_test.pixelwise_test()\n",
    "\n",
    "X_train, y_train = p_train.pixelwise()\n",
    "X_test, y_test = p_test.pixelwise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba330e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../\" # Define it\n",
    "\n",
    "Xmean = np.array([ 596.57817383, 878.493514, 969.89764811, 1324.39628906, 2368.21767578, 2715.68257243, 2886.70323486, 2977.03915609, 2158.25386556, 1462.10965169])\n",
    "Xmean = Xmean.reshape((1, 10, 1, 1))\n",
    "Xstd = np.array([251.33337853, 289.95055489, 438.725014, 398.7289996, 706.53781626, 832.72503267, 898.14189979, 909.04165075, 661.66078257, 529.15340992])\n",
    "Xstd = Xstd.reshape((1, 10, 1, 1))\n",
    "\n",
    "p_train_norm = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                             split=\"train\",\n",
    "                             median_of_days=True,\n",
    "                             Xmean=Xmean,\n",
    "                             Xstd=Xstd,\n",
    "                             binary_labels=False)\n",
    "\n",
    "p_test_norm = PASTISSegmentation(os.path.join(base_path, \"data\", \"images\"),\n",
    "                             os.path.join(base_path, \"data\", \"annotations\"),\n",
    "                            split=\"test\",\n",
    "                            median_of_days=True,\n",
    "                            Xmean=Xmean,\n",
    "                            Xstd=Xstd, \n",
    "                            binary_labels=False)\n",
    "p_train_norm.pixelwise_test()\n",
    "p_test_norm.pixelwise_test()\n",
    "\n",
    "X_train_norm, y_train_norm = p_train_norm.pixelwise()\n",
    "X_test_norm, y_test_norm = p_test_norm.pixelwise()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d53f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of images, we focus on the individual pixels. For each pixel, we have 10 features and a class (in y).\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The normalized data should have the same shape as above\n",
    "X_train_norm.shape, y_train_norm.shape, X_test_norm.shape, y_test_norm.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But the difference in values are strak!\n",
    "print(np.mean(X_train), np.mean(X_train_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = 5\n",
    "# Show the 3rd band of the third image\n",
    "plt.imshow(p_train.X[imd, 3])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Show the labels for third image\n",
    "plt.imshow(p_train.y[imd])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33085bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = 5\n",
    "# Show the 3rd band of the third image\n",
    "plt.imshow(p_train_norm.X[imd, 3])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Show the labels for third image\n",
    "plt.imshow(p_train_norm.y[imd])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d4880",
   "metadata": {},
   "source": [
    "# Feature importance and normalization in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regressor on the unnormalied data\n",
    "logistic_regressor = sklearn.linear_model.LogisticRegression(n_jobs=-1, penalty='none', max_iter=100).fit(X_train, y_train)\n",
    "print(\"Score:\", logistic_regressor.score(X_test, y_test))\n",
    "\n",
    "# Compute feature importance \n",
    "model_fi = permutation_importance(logistic_regressor, X_train, y_train)\n",
    "print(\"Importance of the 10 features:\", model_fi['importances_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regressor on the normalied data\n",
    "logistic_regressor_norm = sklearn.linear_model.LogisticRegression(n_jobs=-1, penalty='none', max_iter=100).fit(X_train_norm, y_train_norm)\n",
    "print(\"Score:\", logistic_regressor_norm.score(X_train_norm, y_train_norm))\n",
    "\n",
    "# Compute feature importance\n",
    "model_fi_norm = permutation_importance(logistic_regressor_norm, X_train_norm, y_train_norm)\n",
    "print(\"Importance of the 10 features:\", model_fi_norm['importances_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81acedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regressor on the unnormalied data for 1000 iterations\n",
    "# This will take a long time!!!\n",
    "logistic_regressor = sklearn.linear_model.LogisticRegression(n_jobs=-1, penalty='none', max_iter=1000).fit(X_train, y_train)\n",
    "print(\"Score:\", logistic_regressor.score(X_test, y_test))\n",
    "\n",
    "# Compute feature importance \n",
    "model_fi = permutation_importance(logistic_regressor, X_train, y_train)\n",
    "print(\"Importance of the 10 features:\", model_fi['importances_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f869f9",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5dcace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest\n",
    "random_forest = sklearn.ensemble.RandomForestClassifier(n_jobs=-1, random_state=0).fit(X_train, y_train)\n",
    "print(\"Score:\", random_forest.score(X_test, y_test))\n",
    "\n",
    "print(\"Importance of the 10 features:\", random_forest.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394525ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest\n",
    "random_forest_norm = sklearn.ensemble.RandomForestClassifier(n_jobs=-1, random_state=0).fit(X_train_norm, y_train_norm)\n",
    "print(\"Score:\", random_forest_norm.score(X_test_norm, y_test_norm))\n",
    "print(\"Importance of the 10 features:\", random_forest_norm.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b090d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
